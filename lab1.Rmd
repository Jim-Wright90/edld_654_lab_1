---
title: "Lab 1: Thuy; Claire; Jim"
subtitle: "Resampling"
date: "Assigned 10/14/20, Due 10/21/20"
output:
  html_document: 
    toc: true
    toc_float: true
    theme: "journal"
    css: "website-custom.css"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(tidymodels)

```

### Read in the `train.csv` data.

```{r, data}
data <- read_csv(here::here("data", "train.csv"))

data <- dplyr::sample_frac(data, size = 0.02)
```

### 1. Initial Split

Split the data into a training set and a testing set as two named objects. Produce the `class` type for the initial split object and the training and test sets.

```{r, initial_split}
set.seed(3000)

(data_split <- initial_split(data))

data_train <- training(data_split)
data_test <- testing(data_split)

class(data_split)
class(data_train)
class(data_test)

```

### 2. Use code to show the proportion of the `train.csv` data that went to each of the training and test sets.

```{r}
set.seed(3000)

data_split %>% training() %>% nrow() /nrow(data)

data_split %>% testing() %>% nrow() /nrow(data)

```

### 3. *k*-fold cross-validation

Use 10-fold cross-validation to resample the training data.

```{r, resample}
set.seed(3000)

(cv_splits <- vfold_cv(data_train))

```

### 4. Use `{purrr}` to add the following columns to your *k*-fold CV object:
* *analysis_n* = the *n* of the analysis set for each fold
* *assessment_n* = the *n* of the assessment set for each fold
* *analysis_p* = the proportion of the analysis set for each fold
* *assessment_p* = the proportion of the assessment set for each fold
* *sped_p* = the proportion of students receiving special education services (`sp_ed_fg`) in the analysis and assessment sets for each fold

```{r, purrr}

cv_splits_2 <- cv_splits %>% 
  mutate(analysis_n = map_dbl(splits, ~nrow(analysis(.x))),
         assessment_n = map_dbl(splits, ~nrow(assessment(.x))),
         analysis_p = analysis_n / (analysis_n + assessment_n),
         assessment_p = assessment_n / (analysis_n + assessment_n))

sped_p_analysis <- map(cv_splits$splits, ~analysis(.x) %>% 
                           janitor::tabyl(sp_ed_fg)) %>% 
  map_dbl(~(.x)$percent[[2]])

sped_p_assessment <- map(cv_splits$splits, ~assessment(.x) %>% 
                           janitor::tabyl(sp_ed_fg)) %>% 
  map_dbl(~(.x)$percent[[2]])

sped_p <- sped_p_analysis/sped_p_assessment

cv_splits_2$sped_p <- sped_p

head(cv_splits_2)
                          
```

### 5. Please demonstrate that that there are **no** common values in the `id` columns of the `assessment` data between `Fold01` & `Fold02`, and `Fold09` & `Fold10` (of your 10-fold cross-validation object).

```{r}

nrow(inner_join(assessment(cv_splits_2$splits[[1]]),
                assessment(cv_splits_2$splits[[2]]),
                by = "id"))

nrow(inner_join(assessment(cv_splits_2$splits[[9]]),
                assessment(cv_splits_2$splits[[10]]),
                by = "id"))
```

### 6. Try to answer these next questions without running similar code on real data.

For the following code `vfold_cv(fictional_train, v = 20)`:

* What is the proportion in the analysis set for each fold?

Answer: The analysis set serves as 95% for each fold. 

* What is the proportion in the assessment set for each fold?

Answer: The assessment set serves as 5% for each fold. 

### 7. Use Monte Carlo CV to resample the training data with 20 resamples and .30 of each resample reserved for the assessment sets.

```{r}
set.seed(3000)
(mc_splits <- mc_cv(data_train, 
                    prop = .70,
                    times = 20))

```

### 8. Please demonstrate that that there **are** common values in the `id` columns of the `assessment` data between `Resample 8` & `Resample 12`, and `Resample 2` & `Resample 20`in your MC CV object.

```{r}

nrow(inner_join(assessment(mc_splits$splits[[8]]),
                assessment(mc_splits$splits[[12]]),
                by = "id"))

nrow(inner_join(assessment(mc_splits$splits[[2]]),
                assessment(mc_splits$splits[[20]]),
                by = "id"))

```

### 9. You plan on doing bootstrap resampling with a training set with *n* = 500.

* 1) What is the sample size of an analysis set for a given bootstrap resample?
Answer: 500 - during each split, all rows go into the analysis set. 
* 2) What is the sample size of an assessment set for a given bootstrap resample?
Answer: approximately 183
* 3) If each row was selected only once for an analysis set:

  + what would be the size of the analysis set?
  Answer: 500
  
  + and what would be the size of the assessment set?
  Answer: approximately 183
  
Jim: These last two confuse me. Looking at the image presented in the slides when there is one round of sampling, it appears 40% would end up in the analysis set and 30% in the assessment set, but I'm not sure that makes much sense?

Claire: for the bootstrap my understanding is, during each split, all the rows go into analysis set (see the code below) so the sample size will be 500 for question 1); for the question 2) I think you're right, it would be approximately 183 but it depends; for the question 3), I am confused as well since the answer would be no difference with 1) and 2)?

```{r}
nrow(data)
boot_splits <- bootstraps(data)
boot_splits$splits[[1]]

boot_splits_once <- bootstraps(data, times = 1)
boot_splits_once$splits[[1]]

```

  
